{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd04pjo3vSlg",
        "outputId": "b60aac2b-eb01-42b9-87ef-afbcba954fcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\""
      ],
      "metadata": {
        "id": "T8voU8G0vV4o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "tokenized_text= sent_tokenize(text)\n",
        "print(tokenized_text)\n",
        "#Word Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_word=word_tokenize(text)\n",
        "print(tokenized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTPxns63v4Jt",
        "outputId": "a1a28c13-8a9c-4ccd-ce9d-0f2e1e492be4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n",
            "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyScr-HZwGX-",
        "outputId": "e35f1314-c62c-4482-a56e-a644e607db02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'only', 'who', 'yourselves', \"i'd\", \"you'd\", \"we'll\", 'more', 'and', 'up', \"we'd\", 'what', \"should've\", 'why', 'which', 'than', 'then', 'should', \"wouldn't\", 'out', 'didn', 'been', 'you', 'but', 'be', \"she'll\", 'ours', 'not', 'she', 'weren', \"i'm\", 'themselves', 'off', 'an', \"it's\", 'mustn', 'shan', 'isn', \"you'll\", 'do', 'during', 'herself', 'own', 'before', 'o', 'some', 'we', 'other', 'd', \"he'll\", \"needn't\", 'this', 'each', 'he', 'him', 'as', 'needn', 'now', 'a', 'don', 'their', 'from', 'theirs', 'these', 'there', \"couldn't\", \"mightn't\", \"won't\", \"you've\", 'ourselves', 'too', 'because', 'on', 'until', \"i'll\", \"shouldn't\", 'the', 'ain', 'are', \"it'd\", 'hers', 'shouldn', 'same', 'won', 'its', \"we're\", 'whom', 'under', 'being', \"they're\", \"didn't\", 'ma', 'my', \"mustn't\", 'am', 'were', 'hadn', 'is', 'mightn', 'that', 'such', 'down', 'so', 'over', 'aren', 'myself', \"aren't\", 'himself', \"hasn't\", 'against', 'those', 'or', 't', 'very', 'y', 'here', 'of', 'our', 'between', 've', 'after', 'by', 'her', 'nor', 'them', 'they', \"they'd\", \"he'd\", 'about', 'with', 'has', \"weren't\", 'at', 'doing', 'if', 'in', \"wasn't\", \"you're\", \"it'll\", 'no', 'all', 'into', 'for', 'itself', 'most', \"isn't\", 'yourself', 'once', \"that'll\", 'your', 'll', \"we've\", 'further', 'having', 'doesn', 'i', 'it', 'when', 'had', 'where', \"don't\", 'did', 'was', 'haven', \"he's\", 'both', 'above', 'below', \"haven't\", 'wasn', \"they've\", 'just', 'will', 'to', 'yours', 'through', \"hadn't\", 'can', 'me', 'how', 'any', \"shan't\", 'again', 's', \"doesn't\", 'while', 'wouldn', 'couldn', 'few', 'does', 'hasn', \"i've\", \"she's\", 'his', 'have', 'm', 're', \"she'd\", \"they'll\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"How to remove stop words with NLTK library in Python?\"\n",
        "text=re.sub('[^a-zA-Z]', ' ',text)\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered_text=[]\n",
        "for w in tokens:\n",
        "  if w not in stop_words:\n",
        "    filtered_text.append(w)\n",
        "print(\"Tokenized Sentence:\",tokens)\n",
        "print(\"Filterd Sentence:\",filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAMVZ2VRxYnP",
        "outputId": "5c541a99-54c5-490f-876c-604a2122442d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
            "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "e_words= [\"wait\", \"waiting\", \"waited\",\"waits\"]\n",
        "ps =PorterStemmer()\n",
        "for w in e_words:\n",
        "  rootWord=ps.stem(w)\n",
        "  print(rootWord)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DllU0LmyxqZF",
        "outputId": "7c1245b4-5ec9-4ee4-c13a-578d811ede87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wait\n",
            "wait\n",
            "wait\n",
            "wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "\n",
        "for w in tokenization:\n",
        "    print(\"Lemma for {}: {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rFaGItu0SfE",
        "outputId": "d6e30fe9-415a-48ca-f83e-a00d7f50da46"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for studies: study\n",
            "Lemma for studying: studying\n",
            "Lemma for cries: cry\n",
            "Lemma for cry: cry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "data = \"The pink sweater fit her perfectly\"\n",
        "words = word_tokenize(data)\n",
        "\n",
        "for word in words:\n",
        "    print(nltk.pos_tag([word]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQCgwZAV0_pE",
        "outputId": "402ae9b9-6eb6-43c2-d68c-097fe1896678"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT')]\n",
            "[('pink', 'NN')]\n",
            "[('sweater', 'NN')]\n",
            "[('fit', 'NN')]\n",
            "[('her', 'PRP$')]\n",
            "[('perfectly', 'RB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import math"
      ],
      "metadata": {
        "id": "oUeEejYv1D-G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentA = 'Jupiter is the largest Planet'\n",
        "documentB = 'Mars is the fourth planet from the Sun'"
      ],
      "metadata": {
        "id": "7Qpq0hir1WWb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bagOfWordsA = documentA.split(' ')\n",
        "bagOfWordsB = documentB.split(' ')"
      ],
      "metadata": {
        "id": "_H7pRXdo1YyV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bagOfWordsA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHTHok2u1bTs",
        "outputId": "59285b7a-6513-47a4-9b6c-9991c675ce2b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jupiter', 'is', 'the', 'largest', 'Planet']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bagOfWordsB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trtcx1eZ1elx",
        "outputId": "87163b10-5856-4555-e99c-a0340c423da6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mars', 'is', 'the', 'fourth', 'planet', 'from', 'the', 'Sun']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
        "uniqueWords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKcMu9pL1gdW",
        "outputId": "de4aecf5-785d-4cdc-f609-fb794afbe7af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Jupiter',\n",
              " 'Mars',\n",
              " 'Planet',\n",
              " 'Sun',\n",
              " 'fourth',\n",
              " 'from',\n",
              " 'is',\n",
              " 'largest',\n",
              " 'planet',\n",
              " 'the'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsA:\n",
        "    numOfWordsA[word] += 1\n",
        "\n",
        "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsB:\n",
        "    numOfWordsB[word] += 1"
      ],
      "metadata": {
        "id": "VXy8ou2s1iv8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numOfWordsA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTGN8TER1lSB",
        "outputId": "a5f98839-e41b-4f42-e905-4937dedf139f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is': 1,\n",
              " 'the': 1,\n",
              " 'from': 0,\n",
              " 'planet': 0,\n",
              " 'fourth': 0,\n",
              " 'Jupiter': 1,\n",
              " 'Planet': 1,\n",
              " 'largest': 1,\n",
              " 'Mars': 0,\n",
              " 'Sun': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numOfWordsB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWPo15U61nrs",
        "outputId": "ddc8a4ad-c462-4faf-8f30-9a1ecd41e0a7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is': 1,\n",
              " 'the': 2,\n",
              " 'from': 1,\n",
              " 'planet': 1,\n",
              " 'fourth': 1,\n",
              " 'Jupiter': 0,\n",
              " 'Planet': 0,\n",
              " 'largest': 0,\n",
              " 'Mars': 1,\n",
              " 'Sun': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTF(wordDict, bagOfWords):\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count / float(bagOfWordsCount)\n",
        "    return tfDict\n",
        "\n",
        "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
        "tfB = computeTF(numOfWordsB, bagOfWordsB)"
      ],
      "metadata": {
        "id": "gfcJ8w3u1o5I"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKzJfESt1sXw",
        "outputId": "3fb488a8-19b6-4536-cf18-fee8cd3b3f5b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is': 0.2,\n",
              " 'the': 0.2,\n",
              " 'from': 0.0,\n",
              " 'planet': 0.0,\n",
              " 'fourth': 0.0,\n",
              " 'Jupiter': 0.2,\n",
              " 'Planet': 0.2,\n",
              " 'largest': 0.2,\n",
              " 'Mars': 0.0,\n",
              " 'Sun': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GDuRE0C1vVs",
        "outputId": "84958b3f-4368-427b-9bfa-684a814d38bb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is': 0.125,\n",
              " 'the': 0.25,\n",
              " 'from': 0.125,\n",
              " 'planet': 0.125,\n",
              " 'fourth': 0.125,\n",
              " 'Jupiter': 0.0,\n",
              " 'Planet': 0.0,\n",
              " 'largest': 0.0,\n",
              " 'Mars': 0.125,\n",
              " 'Sun': 0.125}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def computeIDF(documents):\n",
        "    N = len(documents)\n",
        "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
        "    for document in documents:\n",
        "        for word, val in document.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N / float(val))\n",
        "    return idfDict\n",
        "\n",
        "idfs = computeIDF([numOfWordsA, numOfWordsB])"
      ],
      "metadata": {
        "id": "78cESCuo1weA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcxRpREX1zQr",
        "outputId": "b53fa9ad-ef6d-492d-cc25-c94b39ef09d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is': 0.0,\n",
              " 'the': 0.0,\n",
              " 'from': 0.6931471805599453,\n",
              " 'planet': 0.6931471805599453,\n",
              " 'fourth': 0.6931471805599453,\n",
              " 'Jupiter': 0.6931471805599453,\n",
              " 'Planet': 0.6931471805599453,\n",
              " 'largest': 0.6931471805599453,\n",
              " 'Mars': 0.6931471805599453,\n",
              " 'Sun': 0.6931471805599453}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTFIDF(tfBagOfWords, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBagOfWords.items():\n",
        "        tfidf[word] = val * idfs[word]\n",
        "    return tfidf\n",
        "\n",
        "tfidfA = computeTFIDF(tfA, idfs)\n",
        "tfidfB = computeTFIDF(tfB, idfs)\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "df = pd.DataFrame([tfidfA, tfidfB])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hcwFT-g12d4",
        "outputId": "cf4eb312-bb04-463c-c1a7-566193d1cdac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    is  the      from    planet    fourth   Jupiter    Planet   largest  \\\n",
            "0  0.0  0.0  0.000000  0.000000  0.000000  0.138629  0.138629  0.138629   \n",
            "1  0.0  0.0  0.086643  0.086643  0.086643  0.000000  0.000000  0.000000   \n",
            "\n",
            "       Mars       Sun  \n",
            "0  0.000000  0.000000  \n",
            "1  0.086643  0.086643  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t9NxJTNm15V8"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}